{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3ca150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cb8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "INPUT_PATH = \"/Users/sirrea/workspaces/data_science/saltLM/data/oasst2_single_turn_sft_1200_sampled.jsonl\"\n",
    "OUTPUT_PATH = \"/Users/sirrea/workspaces/data_science/saltLM/data/oasst2_single_turn_sft_1200_rewritten.jsonl\"\n",
    "STATE_PATH = \"rewrite_state.json\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f9abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.2fe192450127e6a83f7441aef6e3ca586c338b77.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.2fe192450127e6a83f7441aef6e3ca586c338b77.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3177b213dbab47c1af8df2b17434f48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eefd2f27e7b4e069c39958ef82beabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959db84f48c64edaa2724c369ea1f355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd879751dd4ef9a2f4a1aca770b703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e53f344b3e4c98a87def41ad76d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76b55d3e19546b8a7b4c36053c87b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b226387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, response):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You rewrite assistant responses.\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"- Preserve exact meaning and factual correctness\\n\"\n",
    "                \"- Do NOT add or remove information\\n\"\n",
    "                \"- Change ONLY tone and personality\\n\"\n",
    "                \"- Tone: snarky, mildly sarcastic, passive-aggressive, competent, high ego\\n\"\n",
    "                \"- No emojis, no roleplay, no meta commentary\\n\"\n",
    "                \"- Output ONLY the rewritten response\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Instruction:\\n{instruction}\\n\\nOriginal response:\\n{response}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def rewrite_batch(batch):\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            build_prompt(row[\"instruction\"], row[\"response\"]),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for row in batch\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return [d.strip() for d in decoded]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345997ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sirrea/workspaces/data_science/saltLM/data/oasst2_single_turn_sft_1200_sampled.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3313422028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Resume state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sirrea/workspaces/data_science/saltLM/data/oasst2_single_turn_sft_1200_sampled.jsonl'"
     ]
    }
   ],
   "source": [
    "# Load full dataset\n",
    "with open(INPUT_PATH) as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Resume state\n",
    "start_idx = 0\n",
    "if os.path.exists(STATE_PATH):\n",
    "    with open(STATE_PATH) as f:\n",
    "        start_idx = json.load(f)[\"next_index\"]\n",
    "\n",
    "mode = \"a\" if start_idx > 0 else \"w\"\n",
    "\n",
    "with open(OUTPUT_PATH, mode) as fout:\n",
    "    for i in tqdm(range(start_idx, len(data), BATCH_SIZE), desc=\"Rewriting batches\"):\n",
    "        batch = data[i : i + BATCH_SIZE]\n",
    "        rewritten = rewrite_batch(batch)\n",
    "\n",
    "        for row, new_text in zip(batch, rewritten):\n",
    "            row[\"candidate_response\"] = new_text\n",
    "            fout.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save resume point\n",
    "        with open(STATE_PATH, \"w\") as f:\n",
    "            json.dump({\"next_index\": i + BATCH_SIZE}, f)\n",
    "\n",
    "        # HARD memory cleanup\n",
    "        del rewritten, batch\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d5637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
