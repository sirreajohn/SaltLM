{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3ca150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cb8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "INPUT_PATH = \"/home/sagemaker-user/SaltLM/data/oasst2_single_turn_sft_1200_sampled.jsonl\"\n",
    "OUTPUT_PATH = \"/home/sagemaker-user/SaltLM/data/oasst2_single_turn_sft_500_rewritten.jsonl\"\n",
    "STATE_PATH = \"rewrite_state.json\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This model config has set a `rope_parameters['original_max_position_embeddings']` field, to be used together with `max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, as it is compatible with most model architectures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1fef79f0fa404a871b9dba1f7f2770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, response):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You rewrite assistant responses.\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"- Preserve exact meaning and factual correctness\\n\"\n",
    "                \"- Do NOT add or remove information\\n\"\n",
    "                \"- Change ONLY tone and personality\\n\"\n",
    "                \"- Tone: snarky, mildly sarcastic, passive-aggressive, competent, high ego\\n\"\n",
    "                \"- No emojis, no roleplay, no meta commentary\\n\"\n",
    "                \"- Output ONLY the rewritten response\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Instruction:\\n{instruction}\\n\\nOriginal response:\\n{response}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def rewrite_batch(batch):\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            build_prompt(row[\"instruction\"], row[\"response\"]),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for row in batch\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return [d.strip() for d in decoded]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "with open(INPUT_PATH) as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Resume state\n",
    "start_idx = 0\n",
    "if os.path.exists(STATE_PATH):\n",
    "    with open(STATE_PATH) as f:\n",
    "        start_idx = json.load(f)[\"next_index\"]\n",
    "\n",
    "mode = \"a\" if start_idx > 0 else \"w\"\n",
    "\n",
    "with open(OUTPUT_PATH, mode) as fout:\n",
    "    for i in tqdm(range(start_idx, len(data), BATCH_SIZE), desc=\"Rewriting batches\"):\n",
    "        batch = data[i : i + BATCH_SIZE]\n",
    "        rewritten = rewrite_batch(batch)\n",
    "\n",
    "        for row, new_text in zip(batch, rewritten):\n",
    "            row[\"candidate_response\"] = new_text\n",
    "            fout.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save resume point\n",
    "        with open(STATE_PATH, \"w\") as f:\n",
    "            json.dump({\"next_index\": i + BATCH_SIZE}, f)\n",
    "\n",
    "        # HARD memory cleanup\n",
    "        del rewritten, batch\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7913268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SaltLM (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
